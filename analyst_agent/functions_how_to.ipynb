{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be99f26-d5df-4a87-b568-9f8cbca3f118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marie/Documents/github/llm_env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d108b5-d04b-4f6f-84b0-1039f0e44cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('/Users/marie/Documents/github/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "    for key in config:\n",
    "        os.environ[key] = config[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc8676-6142-4411-89ad-8b4da3e72948",
   "metadata": {},
   "source": [
    "##Â Extraction example\n",
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672c2171-e07e-4954-92f3-05d8258b8856",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m\n\u001b[1;32m      3\u001b[0m extraction_functions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextract_information\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     }\n\u001b[1;32m     39\u001b[0m ]\n\u001b[1;32m     41\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m     {\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m ]\n\u001b[0;32m---> 52\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-1106\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextraction_functions\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "extraction_functions = [\n",
    "    {\n",
    "        \"name\": \"extract_information\",\n",
    "        \"description\": \"extracts information\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"metric\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"main metric we need to calculate, for example, 'number of users' or 'number of sessions'\",\n",
    "                },\n",
    "                \"filters\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"filters to apply to the calculation (do not include filters on dates here)\",\n",
    "                },\n",
    "                \"dimensions\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"parameters to split you metric by\",\n",
    "                },\n",
    "                \"period_start\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"start day of the period for report\",\n",
    "                },\n",
    "                \"period_end\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"end day of the period for report\",\n",
    "                },\n",
    "                \"output_type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"the desired output\",\n",
    "                    \"enum\": [\"number\", \"visualisation\"]\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"metric\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Extract the relevant information from the provided request. Extract ONLY the information presented in the initial request, don't add anything else. Return partial information if something is missing.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How did number of iOS users change over time?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    messages=messages,\n",
    "    functions=extraction_functions\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4dd4c8d-9b2c-458b-97ed-da73217e44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Optional\n",
    "\n",
    "class RequestStructure(BaseModel):\n",
    "  \"\"\"extracts information\"\"\"\n",
    "  metric: str = Field(description = \"main metric we need to calculate, for example, 'number of users' or 'number of sessions'\")\n",
    "  filters: Optional[str] = Field(description = \"filters to apply to the calculation (do not include filters on dates here)\")\n",
    "  dimensions: Optional[str] = Field(description = \"parameters to split you metric by\")\n",
    "  period_start: Optional[str] = Field(description = \"start day of the period for report\")\n",
    "  period_end: Optional[str] = Field(description = \"end day of the period for report\")\n",
    "  output_type: Optional[str] = Field(description = \"the desired output\", enum = [\"number\", \"visualisation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15b52e93-ef79-4cba-a54a-0be03253653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "extract_info_function = convert_pydantic_to_openai_function(RequestStructure, name = 'extract_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4181e0ed-5331-4c6c-aaa7-bcc51810be3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'extract_information',\n",
       " 'description': 'extracts information',\n",
       " 'parameters': {'description': 'extracts information',\n",
       "  'properties': {'metric': {'description': \"main metric we need to calculate, for example, 'number of users' or 'number of sessions'\",\n",
       "    'title': 'Metric',\n",
       "    'type': 'string'},\n",
       "   'filters': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "    'description': 'filters to apply to the calculation (do not include filters on dates here)',\n",
       "    'title': 'Filters'},\n",
       "   'dimensions': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "    'description': 'parameters to split you metric by',\n",
       "    'title': 'Dimensions'},\n",
       "   'period_start': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "    'description': 'start day of the period for report',\n",
       "    'title': 'Period Start'},\n",
       "   'period_end': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "    'description': 'end day of the period for report',\n",
       "    'title': 'Period End'},\n",
       "   'output_type': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "    'description': 'the desired output',\n",
       "    'enum': ['number', 'visualisation'],\n",
       "    'title': 'Output Type'}},\n",
       "  'required': ['metric',\n",
       "   'filters',\n",
       "   'dimensions',\n",
       "   'period_start',\n",
       "   'period_end',\n",
       "   'output_type'],\n",
       "  'title': 'RequestStructure',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_info_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576e23fc-1ceb-43a2-9cdf-334a996a1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [extract_info_function])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information from the provided request.\"),\n",
    "    (\"human\", \"{request}\")\n",
    "])\n",
    "\n",
    "extraction_chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8939e35f-620f-4af8-afff-dcc2cf8418ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"metric\":\"number of users\",\"filters\":\"device = \\'iOS\\'\",\"dimensions\":\"country\",\"period_start\":\"2023-04-01\",\"period_end\":\"2023-04-30\",\"output_type\":\"number\"}', 'name': 'extract_information'}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({'request': \"How many customers visited our site on iOS in April 2023 from different countries?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7f68ce-3600-494f-bc4d-3345ca36424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='', additional_kwargs={'function_call': {'name': 'extract_information', 'arguments': '{\"metric\":\"number of customers\",\"filters\":\"device = 'iOS'\",\"dimensions\":\"country\",\"period_start\":\"2023-04-01\",\"period_end\":\"2023-04-30\",\"output_type\":\"number\"}'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"AIMessage(content='', additional_kwargs={'function_call': {'name': 'extract_information', 'arguments': '{\"metric\":\"number of customers\",\"filters\":\"device = \\'iOS\\'\",\"dimensions\":\"country\",\"period_start\":\"2023-04-01\",\"period_end\":\"2023-04-30\",\"output_type\":\"number\"}'}}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a7d6c-03ec-4eb2-8268-d270f85641f7",
   "metadata": {},
   "source": [
    "### Built-in tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bbbf41c-7554-46f3-b333-ff5e1de21726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f955f979-edaa-4341-ba94-4eb148140a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5328ea7f-00c7-4c12-9045-0f8a2849d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "# from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2938a50b-faf5-4edd-8dc6-60ed4d71bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_repl = PythonREPL()\n",
    "# python_repl.run(\"print(100*30/40)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479b9986-14fc-4bc8-96e1-c001da1af7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_CSE_ID\"] = \"<id>\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<your_api_key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be362cca-3209-4791-8be8-0b347511f9a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "google-api-python-client is not installed. Please install it with `pip install google-api-python-client>=2.100.0`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/langchain_community/utilities/google_search.py:77\u001b[0m, in \u001b[0;36mGoogleSearchAPIWrapper.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tool\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleSearchAPIWrapper\n\u001b[0;32m----> 4\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mGoogleSearchAPIWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgoole_results\u001b[39m(query):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m search\u001b[38;5;241m.\u001b[39mresults(query, \u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/pydantic/v1/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/langchain_community/utilities/google_search.py:80\u001b[0m, in \u001b[0;36mGoogleSearchAPIWrapper.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle-api-python-client is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install google-api-python-client\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=2.100.0`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     86\u001b[0m service \u001b[38;5;241m=\u001b[39m build(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomsearch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, developerKey\u001b[38;5;241m=\u001b[39mgoogle_api_key)\n\u001b[1;32m     87\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_engine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m service\n",
      "\u001b[0;31mImportError\u001b[0m: google-api-python-client is not installed. Please install it with `pip install google-api-python-client>=2.100.0`"
     ]
    }
   ],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "def goole_results(query):\n",
    "    return search.results(query, 5)\n",
    "\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Google Search\",\n",
    "    description=\"Search Google to get basic information about the world.\",\n",
    "    func=top5_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc2b0f7f-e289-46bf-8491-264501252388",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_tool_to_openai_function\n\u001b[0;32m----> 3\u001b[0m format_tool_to_openai_function(\u001b[43mtool\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tool' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "format_tool_to_openai_function(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01dbf58-52a2-46af-b4a6-13a4ab76fa63",
   "metadata": {},
   "source": [
    "### Custom tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15ad8b00-e29c-426d-8d5c-688c17b938b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a603324c-a045-474d-8bed-43396b3b61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def percentage_difference(metric1: float, metric2: float) -> float:\n",
    "    \"\"\"Calculates the percentage difference between metrics\"\"\"\n",
    "    return (metric2 - metric1)/metric1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a0a6325-7cc0-43e9-a166-7251363791bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'percentage_difference'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_difference.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b401aaa1-478c-4f6f-be54-72e09eedd449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric1': {'title': 'Metric1', 'type': 'number'},\n",
       " 'metric2': {'title': 'Metric2', 'type': 'number'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_difference.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44d19add-6fac-466e-ba20-868c516a6760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'percentage_difference(metric1: float, metric2: float) -> float - Calculates the percentage difference between metrics'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentage_difference.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c682f11-2fbc-4237-ab2d-70f4716bd401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'percentage_difference',\n",
       " 'description': 'percentage_difference(metric1: float, metric2: float) -> float - Calculates the percentage difference between metrics',\n",
       " 'parameters': {'title': 'percentage_differenceSchemaSchema',\n",
       "  'type': 'object',\n",
       "  'properties': {'metric1': {'title': 'Metric1', 'type': 'number'},\n",
       "   'metric2': {'title': 'Metric2', 'type': 'number'}},\n",
       "  'required': ['metric1', 'metric2']}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_tool_to_openai_function(percentage_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "620f40c2-4ce7-47c1-a145-db3d5b6eba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain==0.0.327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67b09976-2b67-4076-80b3-bcaf5fa9a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.2'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic \n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70958fde-3dcf-4d6f-974b-631c12c9c24d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StructuredTool\nargs_schema\n  subclass of BaseModel expected (type=type_error.subclass; expected_class=BaseModel)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m     metric1: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBase metric value to calculate the difference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     metric2: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew metric value that we compare with the baseline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129;43m@tool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mpercentage_difference\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetric1\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric2\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Calculates the percentage difference between metrics\"\"\"\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mmetric1\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/langchain_core/tools.py:841\u001b[0m, in \u001b[0;36mtool.<locals>._partial\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_partial\u001b[39m(func: Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTool:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_with_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/langchain_core/tools.py:801\u001b[0m, in \u001b[0;36mtool.<locals>._make_with_name.<locals>._make_tool\u001b[0;34m(dec_func)\u001b[0m\n\u001b[1;32m    798\u001b[0m     description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infer_schema \u001b[38;5;129;01mor\u001b[39;00m args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStructuredTool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# If someone doesn't want a schema applied, we must treat it as\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# a simple string->string function\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/langchain_core/tools.py:721\u001b[0m, in \u001b[0;36mStructuredTool.from_function\u001b[0;34m(cls, func, coroutine, name, description, return_direct, args_schema, infer_schema, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m infer_schema:\n\u001b[1;32m    720\u001b[0m     _args_schema \u001b[38;5;241m=\u001b[39m create_schema_from_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_function)\n\u001b[0;32m--> 721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_args_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/langchain_core/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/Documents/github/llm_env/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StructuredTool\nargs_schema\n  subclass of BaseModel expected (type=type_error.subclass; expected_class=BaseModel)"
     ]
    }
   ],
   "source": [
    "class Metrics(BaseModel):\n",
    "    metric1: float = Field(description=\"Base metric value to calculate the difference\")\n",
    "    metric2: float = Field(description=\"New metric value that we compare with the baseline\")\n",
    "\n",
    "@tool(args_schema=Metrics)\n",
    "def percentage_difference(metric1: float, metric2: float) -> float:\n",
    "    \"\"\"Calculates the percentage difference between metrics\"\"\"\n",
    "    return (metric2 - metric1)/metric1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "41eaba70-9a26-4172-9b7f-d68c04e9cea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'percentage_difference',\n",
       " 'description': 'percentage_difference(metric1: float, metric2: float) -> float - Calculates the percentage difference between metrics',\n",
       " 'parameters': {'title': 'Metrics',\n",
       "  'type': 'object',\n",
       "  'properties': {'metric1': {'title': 'Metric1',\n",
       "    'description': 'Base metric value to calculate the difference',\n",
       "    'type': 'number'},\n",
       "   'metric2': {'title': 'Metric2',\n",
       "    'description': 'New metric value that we compare with the baseline',\n",
       "    'type': 'number'}},\n",
       "  'required': ['metric1', 'metric2']}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_tool_to_openai_function(percentage_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "45234476-1e56-4402-ad07-40ab42cfc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [format_tool_to_openai_function(percentage_difference)])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts.\"),\n",
    "    (\"user\", \"{request}\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f735234e-15fc-4318-8ac6-aff080359872",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyst_chain.invoke({'request': \"In April we had 100 users and in May only 95. What is difference in percent?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c8eb0f17-539b-4279-a40d-731b8c107a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":95}'}})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3fe5b1ab-284a-47b8-94c4-91fc196835e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f5179c9-36ef-45af-8fd4-9e37c3a8fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyst_chain.invoke({'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9f775678-fd29-4632-aa6c-2887f88cc732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='percentage_difference', tool_input={'metric1': 100, 'metric2': 110}, log=\"\\nInvoking: `percentage_difference` with `{'metric1': 100, 'metric2': 110}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}})])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "563871d8-89da-4ea1-b29b-f2781ffa940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "observation = percentage_difference(result.tool_input)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "52d7289c-28ea-4f2f-a151-9b86838703f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'percentage_difference'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "30d9ba72-7001-4688-90c2-11ba1d348363",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts.\"),\n",
    "    (\"user\", \"There were 100 users in April and 110 users in May. How did the number of users changed?\"),\n",
    "    (\"assistant\", \"Observation: %f\" % observation)\n",
    "]\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [format_tool_to_openai_function(percentage_difference)])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "57992038-950b-41aa-b7ab-557b57af39c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='percentage_difference', tool_input={'metric1': 100, 'metric2': 110}, log=\"\\nInvoking: `percentage_difference` with `{'metric1': 100, 'metric2': 110}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}})])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyst_chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2ee5480a-5a0c-4873-9dad-c7127b9edb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-3.5-turbo-1106')\\\n",
    "  .bind(functions = [format_tool_to_openai_function(percentage_difference)])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information. You need to be accurate so check all basic info you're using at Wikipedia.\"),\n",
    "    (\"user\", \"{request}\"),\n",
    "    MessagesPlaceholder(variable_name=\"observations\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
    "    \"observations\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1867438c-0027-471c-85f6-e2024517fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "44474e0f-41e8-4295-bafb-5bf1c61a4ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='percentage_difference', tool_input={'metric1': 100, 'metric2': 110}, log=\"\\nInvoking: `percentage_difference` with `{'metric1': 100, 'metric2': 110}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}})])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "461468f7-3c70-4254-a10e-41eaf5c49aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}})]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.message_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9a5d68e6-c2e5-44f6-9558-f7467a6d3b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}}),\n",
       " FunctionMessage(content='10.0', name='percentage_difference')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_to_openai_functions([(result1, observation), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "025402e8-4cc3-4c6f-8874-365ab8ba0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = analyst_chain.invoke({\n",
    "    'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "443b9b79-4d46-4dcc-83a2-d21c994dfcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The number of users increased by 10%.'}, log='The number of users increased by 10%.')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "74e08eaa-de6c-470c-a90e-1567df3e3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "84d35c81-799a-4cf8-bb35-a1b80f8822ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\",\n",
      "          \"additional_kwargs\": {\n",
      "            \"function_call\": {\n",
      "              \"name\": \"percentage_difference\",\n",
      "              \"arguments\": \"{\\\"metric1\\\":100,\\\"metric2\\\":110}\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"FunctionMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"name\": \"percentage_difference\",\n",
      "          \"content\": \"10.0\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\\nHuman: There were 100 users in April and 110 users in May. How did the number of users changed?\\nAI: {'name': 'percentage_difference', 'arguments': '{\\\"metric1\\\":100,\\\"metric2\\\":110}'}\\nFunction: 10.0\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] [2.62s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The number of users increased by 10%.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The number of users increased by 10%.\",\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 186,\n",
      "      \"completion_tokens\": 10,\n",
      "      \"total_tokens\": 196\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-1106\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:OpenAIFunctionsAgentOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:OpenAIFunctionsAgentOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"schema\",\n",
      "    \"agent\",\n",
      "    \"AgentFinish\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"return_values\": {\n",
      "      \"output\": \"The number of users increased by 10%.\"\n",
      "    },\n",
      "    \"log\": \"The number of users increased by 10%.\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [2.63s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "result2 = analyst_chain.invoke({\n",
    "    'request': \"There were 100 users in April and 110 users in May. How did the number of users changed?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "865ed6a3-ae8f-4d05-a6bb-ef9b9cc935f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\n",
      "Human: There were 100 users in April and 110 users in May. How did the number of users changed?\n",
      "AI: {'name': 'percentage_difference', 'arguments': '{\"metric1\":100,\"metric2\":110}'}\n",
      "Function: 10.0\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('''{\n",
    "  \"prompts\": [\n",
    "    \"System: You are a product analyst willing to help your product team. You are very strict to the point and accurate. You use only facts, not inventing information.\\nHuman: There were 100 users in April and 110 users in May. How did the number of users changed?\\nAI: {'name': 'percentage_difference', 'arguments': '{\\\"metric1\\\":100,\\\"metric2\\\":110}'}\\nFunction: 10.0\"\n",
    "  ]\n",
    "}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cf89fda5-c035-443e-a71c-03a6e75cf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "\n",
    "class Filters(BaseModel):\n",
    "    month: str = Field(description=\"Month of customer's activity in the format %Y-%m-%d\")\n",
    "    city: Optional[str] = Field(description=\"City of residence for customers (by default no filter)\", \n",
    "                    enum = [\"London\", \"Berlin\", \"Amsterdam\", \"Paris\"])\n",
    "\n",
    "@tool(args_schema=Filters)\n",
    "def get_monthly_active_users(month: str, city: str = None) -> float:\n",
    "    \"\"\"Returns number of active customers for the specified month\"\"\"\n",
    "    dt = datetime.datetime.strptime(month, '%Y-%m-%d')\n",
    "    total = dt.year + 10*dt.month\n",
    "    if city is None:\n",
    "        return total\n",
    "    else:\n",
    "        return total*random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2a4b973b-a323-415d-a047-b5a9e4b87daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5e4432a0-7cc6-47e0-bed2-12e33f13a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "class Wikipedia(BaseModel):\n",
    "    term: str = Field(description=\"Term to search for\")\n",
    "\n",
    "@tool(args_schema=Wikipedia)\n",
    "def get_summary(term: str) -> str:\n",
    "    \"\"\"Returns basic knowledge related to the term from Wikipedia\"\"\"\n",
    "    return wikipedia.summary(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bf6db3c9-ce45-459c-98ee-8bcf109a33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = {\n",
    "    'percentage_difference': percentage_difference,\n",
    "    'get_monthly_active_users': get_monthly_active_users,\n",
    "    'get_summary': get_summary\n",
    "}\n",
    "\n",
    "analyst_functions = [format_tool_to_openai_function(f) for f in toolkit.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a8cb971c-a37e-466e-935b-e5b3002668a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "model = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview')\\\n",
    "  .bind(functions = analyst_functions)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a product analyst willing to help your product team. You are very strict to the point and accurate. \\\n",
    "        You use only information provided in the initial request. \\\n",
    "        If you need to determine some information i.e. what is the name of the capital, you can use Wikipedia.\"),\n",
    "    (\"user\", \"{request}\"),\n",
    "    MessagesPlaceholder(variable_name=\"observations\")\n",
    "])\n",
    "\n",
    "analyst_chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "737dd6a6-305d-4642-ad64-9b3e73a8b0dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result1 = analyst_chain.invoke({\n",
    "#     'request': \"How many users were in April 2023 from Berlin?\",\n",
    "#     \"observations\": []\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "be5fc678-a73c-4538-9392-50b6dc5e6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Germany between April and May 2023?\",\n",
    "    \"observations\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "71779396-09e8-4925-ad61-29ecd413120f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='get_summary', tool_input={'term': 'Capital of Germany'}, log=\"\\nInvoking: `get_summary` with `{'term': 'Capital of Germany'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_summary', 'arguments': '{\"term\":\"Capital of Germany\"}'}})])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9c5e4c58-4ce3-4eaf-b9c7-379a1c10f1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is the  city state of Berlin. It is the seat of the President of Germany, whose official residence is Schloss Bellevue. The Bundesrat (\"federal council\") is the representation of the Federal States (BundeslÃ¤nder) of Germany and has its seat at the former Prussian Herrenhaus (House of Lords). Though most of the ministries are seated in Berlin, some of them, as well as some minor departments, are seated in Bonn, the former capital of West Germany.\n",
      "Although Berlin is officially the capital of the Federal Republic of Germany, 8,000 out of the 18,000 total officials employed at the federal bureaucracy still work in Bonn, about 600 km (370 mi) away from Berlin.\n"
     ]
    }
   ],
   "source": [
    "observation1 = toolkit[result1.tool](result1.tool_input)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0552c20a-a3e2-470d-a7ea-8805b3570061",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Germany between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation1)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "12df030a-fd74-4190-ac3b-9862a33c3364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='get_monthly_active_users', tool_input={'month': '2023-04-01', 'city': 'Berlin'}, log=\"\\nInvoking: `get_monthly_active_users` with `{'month': '2023-04-01', 'city': 'Berlin'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_monthly_active_users', 'arguments': '{\"month\":\"2023-04-01\",\"city\":\"Berlin\"}'}})])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d9d8796a-b6ca-4ada-b255-528ced3e7e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167.85816148955004\n"
     ]
    }
   ],
   "source": [
    "observation2 = toolkit[result2.tool](result2.tool_input)\n",
    "print(observation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8899bedf-2b4e-429a-b228-0082e0d3da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Germany between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions([(result1, observation1), (result2, observation2)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5a91e336-d4fb-4e45-963c-f912a3485927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='get_monthly_active_users', tool_input={'month': '2023-05-01', 'city': 'Berlin'}, log=\"\\nInvoking: `get_monthly_active_users` with `{'month': '2023-05-01', 'city': 'Berlin'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_monthly_active_users', 'arguments': '{\"month\":\"2023-05-01\",\"city\":\"Berlin\"}'}})])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "519c8a60-b03a-4b7a-aa10-a69b5192981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046.2141109179674\n"
     ]
    }
   ],
   "source": [
    "observation3 = toolkit[result3.tool](result3.tool_input)\n",
    "print(observation3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3a1f0dab-da9f-4779-bdc7-785a1db0fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Germany between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions(\n",
    "      [(result1, observation1), (result2, observation2), \n",
    "      (result3, observation3)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3e4bc70f-cc8a-43d6-98c6-b405f0556ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='percentage_difference', tool_input={'metric1': 167.85816148955004, 'metric2': 1046.2141109179674}, log=\"\\nInvoking: `percentage_difference` with `{'metric1': 167.85816148955004, 'metric2': 1046.2141109179674}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'name': 'percentage_difference', 'arguments': '{\"metric1\":167.85816148955004,\"metric2\":1046.2141109179674}'}})])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "136c79db-0df7-474e-ba7f-d35b78d34b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523.2727093124389\n"
     ]
    }
   ],
   "source": [
    "observation4 = toolkit[result4.tool](result4.tool_input)\n",
    "print(observation4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "facb1bfa-e40f-4a0a-8726-fa9365247fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = analyst_chain.invoke({\n",
    "    'request': \"How did the number of users from the capital of Germany between April and May 2023?\",\n",
    "    \"observations\": format_to_openai_functions(\n",
    "      [(result1, observation1), (result2, observation2), \n",
    "      (result3, observation3), (result4, observation4)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9efc76a5-44bb-4bb3-b545-833f072094fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The number of users from Berlin, the capital of Germany, increased by approximately 523.27% between April and May 2023.'}, log='The number of users from Berlin, the capital of Germany, increased by approximately 523.27% between April and May 2023.')"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5ad5a-3af6-40ab-abda-ec393fdbb37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
